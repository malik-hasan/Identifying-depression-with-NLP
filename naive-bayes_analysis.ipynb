{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using Naive Bayes algorithm\n",
    "\n",
    "(yielded a decent result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Tokenization Code\n",
    "\n",
    "# check  raw text (may have to fix)\n",
    "\n",
    "#1) whitespace tokenizer \n",
    "# bad on username, date, urls\n",
    "\n",
    "#2) treebank-style tokenizer\n",
    "# most popular; hashes text; divides contarctions (eg, can't)\n",
    "#breaks hashtags; breaks emojis; btreaks urls\n",
    "#not best; breaks user ids\n",
    "\n",
    "#want sentiment-aware tokenizer (eg, caps, !, bold, etc)\n",
    "#3) Sentiment-aware tokenizer\n",
    "#preserves hashtags, emojis, user id, etc.\n",
    "#higher accuracy (10k cross-validation) than other tokenizers \n",
    "\n",
    "#this does what is mentioned above\n",
    "#better than treebanck for social media \n",
    "from nltk.tokenize.casual import casual_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600498\n",
      "   target  id                          date     flag      user  \\\n",
      "0       4   3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
      "1       4   4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
      "2       4   5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
      "3       4   6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
      "4       4   7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
      "\n",
      "                                                text  \\\n",
      "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...   \n",
      "1  Reading my kindle2...  Love it... Lee childs i...   \n",
      "2  Ok, first assesment of the #kindle2 ...it fuck...   \n",
      "3  @kenburbary You'll love your Kindle2. I've had...   \n",
      "4  @mikefish  Fair enough. But i have the Kindle2...   \n",
      "\n",
      "                                               words  num_words  \n",
      "0  (I, loooooooovvvvvveee, my, Kindle, 2, ., Not,...         23  \n",
      "1  (Reading, my, kindle, 2, ..., Love, it, ..., L...         14  \n",
      "2  (Ok, ,, first, assesment, of, the, #kindle2, ....         14  \n",
      "3  (You'll, love, your, Kindle, 2, ., I've, had, ...         31  \n",
      "4  (Fair, enough, ., But, i, have, the, Kindle, 2...         15  \n"
     ]
    }
   ],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('../Downloads/sentiment140.csv', header = None, encoding = \"ISO-8859-1\")\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text'] #set column names\n",
    "print(len(df))\n",
    "\n",
    "#add column with tweets tokenized\n",
    "df[\"words\"] = [tuple([word for word in casual_tokenize(row.text) if not re.match('@', \n",
    "  word)]) for _, row in df.iterrows()]\n",
    "df[\"num_words\"] = [len(row.words) for _, row in df.iterrows()]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440448 160050\n"
     ]
    }
   ],
   "source": [
    "train = df.sample(frac=.9)\n",
    "test = df.drop(train.index)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a vocabulary dictionary and some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 33s, sys: 10.9 s, total: 15min 44s\n",
      "Wall time: 16min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#build vocabulary dictionary. Format:  {word : [count|neg, count|neu, count|pos]}\n",
    "#add-1 smoothing: add 1 extra instance of each word so there are no zeroes\n",
    "vocab = {}\n",
    "n_neg, n_neu, n_pos = 0, 0, 0 #number of tweets in each category\n",
    "for _, tweet in train.iterrows():\n",
    "    for word in tweet.words:\n",
    "        if word in vocab:\n",
    "            if tweet.target == 0:\n",
    "                vocab[word][0] += 1\n",
    "            elif tweet.target == 2:\n",
    "                vocab[word][1] += 1\n",
    "            else:\n",
    "                vocab[word][2] += 1\n",
    "        elif not re.match('@', word):\n",
    "            if tweet.target == 0:\n",
    "                vocab[word] = [2, 1, 1]\n",
    "            elif tweet.target == 2:\n",
    "                vocab[word] = [1, 2, 1]\n",
    "            else:\n",
    "                vocab[word] = [1, 1, 2]\n",
    "    if tweet.target == 0:\n",
    "        n_neg += 1\n",
    "    elif tweet.target == 2:\n",
    "        n_neu += 1\n",
    "    else:\n",
    "        n_pos += 1\n",
    "        \n",
    "#log probability of tweet being given category\n",
    "p_neg = math.log(n_neg / len(train))\n",
    "p_neu = math.log(n_neu / len(train))\n",
    "p_pos = math.log(n_pos / len(train))\n",
    "\n",
    "#total amount of words in each category plus 1 placeholder for unknown words\n",
    "negtotal = 1 + sum([value[0] for  value in vocab.values()])\n",
    "neutotal = 1 + sum([value[1] for  value in vocab.values()])\n",
    "postotal = 1 + sum([value[2] for  value in vocab.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(words):\n",
    "    \"\"\"classifies a tweet's sentiment\n",
    "    Formula: Prob(tweet has x sentiment) = Product of prob(word|x) for each word in tweet * prob(x)\"\"\" \n",
    "    neg, neu, pos = p_neg, p_neu, p_pos\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            neg += math.log(vocab[word][0] / negtotal)\n",
    "            neu += math.log(vocab[word][1] / neutotal)\n",
    "            pos += math.log(vocab[word][2] / postotal)            \n",
    "        else:\n",
    "            neg += math.log(1 / negtotal)\n",
    "            neu += math.log(1 / neutotal)\n",
    "            pos += math.log(1 / postotal)\n",
    "    probs = (neg, neu, pos)\n",
    "    if max(probs) == neg:\n",
    "        return 0\n",
    "    elif max(probs) == neu:\n",
    "        return 2\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "def nb_accuracy(test):\n",
    "    \"\"\"tests model and returns accuracy\"\"\"\n",
    "    correct = 0\n",
    "    for _, tweet in test.iterrows():\n",
    "        prediction = classify(tweet.words)\n",
    "        if prediction == tweet.target:\n",
    "            correct += 1\n",
    "    return correct / len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7826616682286786\n",
      "CPU times: user 34.2 s, sys: 1.24 s, total: 35.5 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(nb_accuracy(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
